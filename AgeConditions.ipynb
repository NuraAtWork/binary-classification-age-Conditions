{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ed474531",
   "metadata": {},
   "source": [
    "# Summary\n",
    "The dataset for this project includes data on 617 people and their 57 health characteristics, with one of the columns indicating whether they have a certain health condition associated with aging or not. The goal of the competition, which this dataset was originally a part of, was to build a classification model which would predict whether another person has or does not have this same condition based on his characteristics, which could be used as a powerful tool to help doctors in their diagnostic work. The aim of this notebook, thus, is to recreate this competition and deliver possible models for the task. 5 basic models where employed, including Logistic Regression, Random Forest Clasifier, Support Vector Machine, Naive Bayes, XGBoost, with further hypertuning of top 3 models. In the end, Random Forest was established as the best choice for the problem with 96.96% validation accuracy, but it would make sense to try out the model on more data and test more sophisticated approaches as well."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ceda6e4c",
   "metadata": {
    "papermill": {
     "duration": 0.009647,
     "end_time": "2023-07-11T07:55:04.363121",
     "exception": false,
     "start_time": "2023-07-11T07:55:04.353474",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# 1 Data preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6deb3d64",
   "metadata": {},
   "source": [
    "Before we actually work on our models we will need to prepare the dataset.\n",
    "\n",
    "<b> 1.1 </b> In this step I make import needed modules for the project as well as the dataset itself\n",
    "\n",
    "<b> 1.2 </b> After that I start cleaning my data by encoding categorical data, checking for duplicates and missing entries. For characteristics with a few missing entries I just drop the entries alltogether, luckily it is just 5 entries. For columns with more missing data (around 10% in both'BQ', 'EL') I decided to regress missing information based on values from other columns. Finally, I start dimensionality reduction by looking at pairs of features which correlate the most, leaving only one feature out of the pair in the final dataset, which allows as to drop 13 additional features.\n",
    "\n",
    "<b> 1.3 </b> Normally, I would start the exploratory process here, however, the data we have was produced as a result of Principal Component Analysis, therefore the features we have now are nearly non-interpretable. Instead, I go straight to preparing the dataset for future training by scaling all features (helps some models train faster by removing bias towards columns with biggest values). After that I separate the data into training and testing sets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0980e19",
   "metadata": {
    "papermill": {
     "duration": 0.008769,
     "end_time": "2023-07-11T07:55:04.381552",
     "exception": false,
     "start_time": "2023-07-11T07:55:04.372783",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 1.1 Importing modules and reading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bb7c3015",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-07-11T07:55:04.401906Z",
     "iopub.status.busy": "2023-07-11T07:55:04.401379Z",
     "iopub.status.idle": "2023-07-11T07:55:06.863592Z",
     "shell.execute_reply": "2023-07-11T07:55:06.862422Z"
    },
    "papermill": {
     "duration": 2.475846,
     "end_time": "2023-07-11T07:55:06.866436",
     "exception": false,
     "start_time": "2023-07-11T07:55:04.390590",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Pandas as a main tool to work with dataset\n",
    "import pandas as pd\n",
    "\n",
    "# Libraries for Data preprocessing, Cross-Validation and Accuracy measurements\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "\n",
    "# Importing our Machine Learning models for the classification tasks, \n",
    "# as well as linear regression model which will be used to regress missing data in the columns\n",
    "from sklearn.linear_model import LinearRegression, LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.naive_bayes import GaussianNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "eada6ed3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-07-11T07:55:06.892266Z",
     "iopub.status.busy": "2023-07-11T07:55:06.890878Z",
     "iopub.status.idle": "2023-07-11T07:55:06.987307Z",
     "shell.execute_reply": "2023-07-11T07:55:06.985865Z"
    },
    "papermill": {
     "duration": 0.113628,
     "end_time": "2023-07-11T07:55:06.990620",
     "exception": false,
     "start_time": "2023-07-11T07:55:06.876992",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(617, 58)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>AB</th>\n",
       "      <th>AF</th>\n",
       "      <th>AH</th>\n",
       "      <th>AM</th>\n",
       "      <th>AR</th>\n",
       "      <th>AX</th>\n",
       "      <th>AY</th>\n",
       "      <th>AZ</th>\n",
       "      <th>BC</th>\n",
       "      <th>BD</th>\n",
       "      <th>...</th>\n",
       "      <th>FL</th>\n",
       "      <th>FR</th>\n",
       "      <th>FS</th>\n",
       "      <th>GB</th>\n",
       "      <th>GE</th>\n",
       "      <th>GF</th>\n",
       "      <th>GH</th>\n",
       "      <th>GI</th>\n",
       "      <th>GL</th>\n",
       "      <th>Class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.209377</td>\n",
       "      <td>3109.03329</td>\n",
       "      <td>85.200147</td>\n",
       "      <td>22.394407</td>\n",
       "      <td>8.138688</td>\n",
       "      <td>0.699861</td>\n",
       "      <td>0.025578</td>\n",
       "      <td>9.812214</td>\n",
       "      <td>5.555634</td>\n",
       "      <td>4126.58731</td>\n",
       "      <td>...</td>\n",
       "      <td>7.298162</td>\n",
       "      <td>1.73855</td>\n",
       "      <td>0.094822</td>\n",
       "      <td>11.339138</td>\n",
       "      <td>72.611063</td>\n",
       "      <td>2003.810319</td>\n",
       "      <td>22.136229</td>\n",
       "      <td>69.834944</td>\n",
       "      <td>0.120343</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.145282</td>\n",
       "      <td>978.76416</td>\n",
       "      <td>85.200147</td>\n",
       "      <td>36.968889</td>\n",
       "      <td>8.138688</td>\n",
       "      <td>3.632190</td>\n",
       "      <td>0.025578</td>\n",
       "      <td>13.517790</td>\n",
       "      <td>1.229900</td>\n",
       "      <td>5496.92824</td>\n",
       "      <td>...</td>\n",
       "      <td>0.173229</td>\n",
       "      <td>0.49706</td>\n",
       "      <td>0.568932</td>\n",
       "      <td>9.292698</td>\n",
       "      <td>72.611063</td>\n",
       "      <td>27981.562750</td>\n",
       "      <td>29.135430</td>\n",
       "      <td>32.131996</td>\n",
       "      <td>21.978000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.470030</td>\n",
       "      <td>2635.10654</td>\n",
       "      <td>85.200147</td>\n",
       "      <td>32.360553</td>\n",
       "      <td>8.138688</td>\n",
       "      <td>6.732840</td>\n",
       "      <td>0.025578</td>\n",
       "      <td>12.824570</td>\n",
       "      <td>1.229900</td>\n",
       "      <td>5135.78024</td>\n",
       "      <td>...</td>\n",
       "      <td>7.709560</td>\n",
       "      <td>0.97556</td>\n",
       "      <td>1.198821</td>\n",
       "      <td>37.077772</td>\n",
       "      <td>88.609437</td>\n",
       "      <td>13676.957810</td>\n",
       "      <td>28.022851</td>\n",
       "      <td>35.192676</td>\n",
       "      <td>0.196941</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.252107</td>\n",
       "      <td>3819.65177</td>\n",
       "      <td>120.201618</td>\n",
       "      <td>77.112203</td>\n",
       "      <td>8.138688</td>\n",
       "      <td>3.685344</td>\n",
       "      <td>0.025578</td>\n",
       "      <td>11.053708</td>\n",
       "      <td>1.229900</td>\n",
       "      <td>4169.67738</td>\n",
       "      <td>...</td>\n",
       "      <td>6.122162</td>\n",
       "      <td>0.49706</td>\n",
       "      <td>0.284466</td>\n",
       "      <td>18.529584</td>\n",
       "      <td>82.416803</td>\n",
       "      <td>2094.262452</td>\n",
       "      <td>39.948656</td>\n",
       "      <td>90.493248</td>\n",
       "      <td>0.155829</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.380297</td>\n",
       "      <td>3733.04844</td>\n",
       "      <td>85.200147</td>\n",
       "      <td>14.103738</td>\n",
       "      <td>8.138688</td>\n",
       "      <td>3.942255</td>\n",
       "      <td>0.054810</td>\n",
       "      <td>3.396778</td>\n",
       "      <td>102.151980</td>\n",
       "      <td>5728.73412</td>\n",
       "      <td>...</td>\n",
       "      <td>8.153058</td>\n",
       "      <td>48.50134</td>\n",
       "      <td>0.121914</td>\n",
       "      <td>16.408728</td>\n",
       "      <td>146.109943</td>\n",
       "      <td>8524.370502</td>\n",
       "      <td>45.381316</td>\n",
       "      <td>36.262628</td>\n",
       "      <td>0.096614</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 57 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         AB          AF          AH         AM        AR        AX        AY  \\\n",
       "0  0.209377  3109.03329   85.200147  22.394407  8.138688  0.699861  0.025578   \n",
       "1  0.145282   978.76416   85.200147  36.968889  8.138688  3.632190  0.025578   \n",
       "2  0.470030  2635.10654   85.200147  32.360553  8.138688  6.732840  0.025578   \n",
       "3  0.252107  3819.65177  120.201618  77.112203  8.138688  3.685344  0.025578   \n",
       "4  0.380297  3733.04844   85.200147  14.103738  8.138688  3.942255  0.054810   \n",
       "\n",
       "          AZ          BC         BD   ...        FL        FR        FS  \\\n",
       "0   9.812214    5.555634  4126.58731  ...  7.298162   1.73855  0.094822   \n",
       "1  13.517790    1.229900  5496.92824  ...  0.173229   0.49706  0.568932   \n",
       "2  12.824570    1.229900  5135.78024  ...  7.709560   0.97556  1.198821   \n",
       "3  11.053708    1.229900  4169.67738  ...  6.122162   0.49706  0.284466   \n",
       "4   3.396778  102.151980  5728.73412  ...  8.153058  48.50134  0.121914   \n",
       "\n",
       "          GB          GE            GF         GH         GI         GL  Class  \n",
       "0  11.339138   72.611063   2003.810319  22.136229  69.834944   0.120343      1  \n",
       "1   9.292698   72.611063  27981.562750  29.135430  32.131996  21.978000      0  \n",
       "2  37.077772   88.609437  13676.957810  28.022851  35.192676   0.196941      0  \n",
       "3  18.529584   82.416803   2094.262452  39.948656  90.493248   0.155829      0  \n",
       "4  16.408728  146.109943   8524.370502  45.381316  36.262628   0.096614      1  \n",
       "\n",
       "[5 rows x 57 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv('train.csv')\n",
    "print(data.shape)\n",
    "\n",
    "data = data.drop('Id', axis=1)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "948c9030",
   "metadata": {
    "papermill": {
     "duration": 0.010117,
     "end_time": "2023-07-11T07:55:07.010351",
     "exception": false,
     "start_time": "2023-07-11T07:55:07.000234",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 1.2 Data cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "321852e6",
   "metadata": {
    "papermill": {
     "duration": 0.009411,
     "end_time": "2023-07-11T07:55:07.029749",
     "exception": false,
     "start_time": "2023-07-11T07:55:07.020338",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Checking for duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8727439b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-07-11T07:55:07.051477Z",
     "iopub.status.busy": "2023-07-11T07:55:07.051004Z",
     "iopub.status.idle": "2023-07-11T07:55:07.080106Z",
     "shell.execute_reply": "2023-07-11T07:55:07.078556Z"
    },
    "papermill": {
     "duration": 0.043902,
     "end_time": "2023-07-11T07:55:07.083371",
     "exception": false,
     "start_time": "2023-07-11T07:55:07.039469",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of duplicates: 0\n"
     ]
    }
   ],
   "source": [
    "duplicates = data.duplicated()\n",
    "print(\"Number of duplicates:\", duplicates.sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95e49c74",
   "metadata": {
    "papermill": {
     "duration": 0.014069,
     "end_time": "2023-07-11T07:55:07.107816",
     "exception": false,
     "start_time": "2023-07-11T07:55:07.093747",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Categorical encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4dcdf8b4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-07-11T07:55:07.137853Z",
     "iopub.status.busy": "2023-07-11T07:55:07.137421Z",
     "iopub.status.idle": "2023-07-11T07:55:07.149109Z",
     "shell.execute_reply": "2023-07-11T07:55:07.147725Z"
    },
    "papermill": {
     "duration": 0.027852,
     "end_time": "2023-07-11T07:55:07.151666",
     "exception": false,
     "start_time": "2023-07-11T07:55:07.123814",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create a sample DataFrame\n",
    "data['EJ'] = data['EJ'].replace('A', 0)\n",
    "data['EJ'] = data['EJ'].replace('B', 1)\n",
    "\n",
    "# Convert 'Column1' from object to integer\n",
    "data['EJ'] = data['EJ'].astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c5a63b9",
   "metadata": {
    "papermill": {
     "duration": 0.00952,
     "end_time": "2023-07-11T07:55:07.174953",
     "exception": false,
     "start_time": "2023-07-11T07:55:07.165433",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Dealing with insignificant missing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2806a2a8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-07-11T07:55:07.201438Z",
     "iopub.status.busy": "2023-07-11T07:55:07.200832Z",
     "iopub.status.idle": "2023-07-11T07:55:07.306962Z",
     "shell.execute_reply": "2023-07-11T07:55:07.306069Z"
    },
    "papermill": {
     "duration": 0.124816,
     "end_time": "2023-07-11T07:55:07.309453",
     "exception": false,
     "start_time": "2023-07-11T07:55:07.184637",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of missing values: 131\n",
      "Shape of the data before dropping NULL values: (617, 57)\n",
      "Shape of the data before after NULL values: (612, 57)\n"
     ]
    }
   ],
   "source": [
    "missing_values = data.isna().sum().sum()\n",
    "print(\"Number of missing values:\", missing_values)\n",
    "\n",
    "print(\"Shape of the data before dropping NULL values:\", data.shape)\n",
    "\n",
    "missing_names = []\n",
    "for column_name, column_data in data.items():\n",
    "    missing_percentage = (column_data.isnull().sum() / len(column_data)) * 100\n",
    "    if missing_percentage < 9:\n",
    "        data.dropna(subset=[column_name], inplace=True)\n",
    "    else:\n",
    "        missing_names.append(column_name) \n",
    "        \n",
    "\n",
    "print(\"Shape of the data before after NULL values:\", data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4730a6a7",
   "metadata": {
    "papermill": {
     "duration": 0.009667,
     "end_time": "2023-07-11T07:55:07.329323",
     "exception": false,
     "start_time": "2023-07-11T07:55:07.319656",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Separating responses from predictors before regressing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "604578cb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-07-11T07:55:07.351708Z",
     "iopub.status.busy": "2023-07-11T07:55:07.351079Z",
     "iopub.status.idle": "2023-07-11T07:55:07.357131Z",
     "shell.execute_reply": "2023-07-11T07:55:07.355829Z"
    },
    "papermill": {
     "duration": 0.02005,
     "end_time": "2023-07-11T07:55:07.359644",
     "exception": false,
     "start_time": "2023-07-11T07:55:07.339594",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['BQ', 'EL']\n"
     ]
    }
   ],
   "source": [
    "y = data['Class']  # Extract the response variable column(s)\n",
    "X = data.copy()  # Remove the response variable column(s) from the DataFrame\n",
    "\n",
    "print(missing_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0076b105",
   "metadata": {
    "papermill": {
     "duration": 0.010003,
     "end_time": "2023-07-11T07:55:07.379585",
     "exception": false,
     "start_time": "2023-07-11T07:55:07.369582",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Regressing missing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "05806f5f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-07-11T07:55:07.402426Z",
     "iopub.status.busy": "2023-07-11T07:55:07.401982Z",
     "iopub.status.idle": "2023-07-11T07:55:07.456558Z",
     "shell.execute_reply": "2023-07-11T07:55:07.454910Z"
    },
    "papermill": {
     "duration": 0.071006,
     "end_time": "2023-07-11T07:55:07.461073",
     "exception": false,
     "start_time": "2023-07-11T07:55:07.390067",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Regressing missing BQ data\n",
    "\n",
    "train_X_BQ = X.drop('EL', axis=1)\n",
    "full_BQ = train_X_BQ.copy()\n",
    "test_X_BQ = train_X_BQ[train_X_BQ['BQ'].isnull()].copy()\n",
    "test_X_BQ = test_X_BQ.drop('BQ', axis=1)\n",
    "\n",
    "train_X_BQ.dropna(inplace=True)\n",
    "\n",
    "train_Y_BQ = train_X_BQ['BQ']\n",
    "train_X_BQ = train_X_BQ.drop('BQ', axis=1)\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "regressor = LinearRegression()\n",
    "regressor.fit(train_X_BQ, train_Y_BQ)\n",
    "test_Y_BQ = regressor.predict(test_X_BQ)\n",
    "\n",
    "full_BQ.loc[full_BQ['BQ'].isnull(), 'BQ'] = test_Y_BQ\n",
    "X['BQ'].fillna(full_BQ['BQ'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ed7640e8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-07-11T07:55:07.505149Z",
     "iopub.status.busy": "2023-07-11T07:55:07.504464Z",
     "iopub.status.idle": "2023-07-11T07:55:07.545726Z",
     "shell.execute_reply": "2023-07-11T07:55:07.544181Z"
    },
    "papermill": {
     "duration": 0.068541,
     "end_time": "2023-07-11T07:55:07.550261",
     "exception": false,
     "start_time": "2023-07-11T07:55:07.481720",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Regressing missing EL data\n",
    "\n",
    "train_X_EL = X.copy()\n",
    "full_EL = train_X_EL.copy()\n",
    "test_X_EL = train_X_EL[train_X_EL['EL'].isnull()].copy()\n",
    "test_X_EL = test_X_EL.drop('EL', axis=1)\n",
    "\n",
    "train_X_EL.dropna(inplace=True)\n",
    "\n",
    "train_Y_EL = train_X_EL['EL']\n",
    "train_X_EL = train_X_EL.drop('EL', axis=1)\n",
    "\n",
    "regressor_EL = LinearRegression()\n",
    "regressor_EL.fit(train_X_EL, train_Y_EL)\n",
    "test_Y_EL = regressor_EL.predict(test_X_EL)\n",
    "\n",
    "full_EL.loc[full_EL['EL'].isnull(), 'EL'] = test_Y_EL\n",
    "X['EL'].fillna(full_EL['EL'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3e1acab8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-07-11T07:55:07.594574Z",
     "iopub.status.busy": "2023-07-11T07:55:07.593889Z",
     "iopub.status.idle": "2023-07-11T07:55:07.630114Z",
     "shell.execute_reply": "2023-07-11T07:55:07.628437Z"
    },
    "papermill": {
     "duration": 0.0639,
     "end_time": "2023-07-11T07:55:07.634794",
     "exception": false,
     "start_time": "2023-07-11T07:55:07.570894",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns with low correlation (absolute value < 0.05):\n",
      "AH: 0.04209024672535381\n",
      "AZ: 0.01270428917437259\n",
      "CB: 0.025776691867449207\n",
      "CH: 0.0033435657242242853\n",
      "CL: 0.014922630449296549\n",
      "CS: 0.04964172746379496\n",
      "DN: 0.027359905615540175\n",
      "DV: 0.012521335596920034\n",
      "EG: 0.0343881237284732\n",
      "EU: 0.040800103477387994\n",
      "FC: 0.030450532282503075\n",
      "FS: 0.0011210074041133038\n",
      "GH: 0.027651325525788365\n",
      "\n",
      "Columns with high correlation (absolute value > 0.8):\n",
      "Class: 1.0\n"
     ]
    }
   ],
   "source": [
    "X.columns.tolist()\n",
    "correlation_matrix = X.corr()\n",
    "\n",
    "class_correlations = correlation_matrix['Class']\n",
    "\n",
    "low_correlation_columns = class_correlations[abs(class_correlations) < 0.05]\n",
    "\n",
    "high_correlation_columns = class_correlations[abs(class_correlations) > 0.8]\n",
    "\n",
    "print(\"Columns with low correlation (absolute value < 0.05):\")\n",
    "for column, correlation in low_correlation_columns.items():\n",
    "    print(f\"{column}: {abs(correlation)}\")\n",
    "\n",
    "print(\"\\nColumns with high correlation (absolute value > 0.8):\")\n",
    "for column, correlation in high_correlation_columns.items():\n",
    "    print(f\"{column}: {abs(correlation)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5c8b2563",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-07-11T07:55:07.667555Z",
     "iopub.status.busy": "2023-07-11T07:55:07.667131Z",
     "iopub.status.idle": "2023-07-11T07:55:07.674346Z",
     "shell.execute_reply": "2023-07-11T07:55:07.672854Z"
    },
    "papermill": {
     "duration": 0.021118,
     "end_time": "2023-07-11T07:55:07.676702",
     "exception": false,
     "start_time": "2023-07-11T07:55:07.655584",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "col_drops = ['AH', 'AZ','CB', 'CH','CL','CS','DN','DV','EG','EU','FC','FS','GH'] \n",
    "X.drop(col_drops,axis=1,inplace=True) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01872d3b",
   "metadata": {
    "papermill": {
     "duration": 0.009833,
     "end_time": "2023-07-11T07:55:07.787320",
     "exception": false,
     "start_time": "2023-07-11T07:55:07.777487",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 1.3 Preparing for training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90230fc6",
   "metadata": {
    "papermill": {
     "duration": 0.009921,
     "end_time": "2023-07-11T07:55:07.697438",
     "exception": false,
     "start_time": "2023-07-11T07:55:07.687517",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Normalizing and Splitting data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4cd03102",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-07-11T07:55:07.720247Z",
     "iopub.status.busy": "2023-07-11T07:55:07.719400Z",
     "iopub.status.idle": "2023-07-11T07:55:07.731731Z",
     "shell.execute_reply": "2023-07-11T07:55:07.730834Z"
    },
    "papermill": {
     "duration": 0.026716,
     "end_time": "2023-07-11T07:55:07.734286",
     "exception": false,
     "start_time": "2023-07-11T07:55:07.707570",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "X = X.drop('Class', axis=1)\n",
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1c22e3d7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-07-11T07:55:07.756732Z",
     "iopub.status.busy": "2023-07-11T07:55:07.756267Z",
     "iopub.status.idle": "2023-07-11T07:55:07.764720Z",
     "shell.execute_reply": "2023-07-11T07:55:07.763814Z"
    },
    "papermill": {
     "duration": 0.022704,
     "end_time": "2023-07-11T07:55:07.767132",
     "exception": false,
     "start_time": "2023-07-11T07:55:07.744428",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split \n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bf52d67",
   "metadata": {
    "papermill": {
     "duration": 0.009618,
     "end_time": "2023-07-11T07:55:07.806780",
     "exception": false,
     "start_time": "2023-07-11T07:55:07.797162",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# 2 Training models\n",
    "Now that we have prepared detasets we can start training models. Logistic Regression, Random Forest Clasifier, Support Vector Machine, Naive Bayes, XGBoost were chosen as most popular regression choices. In the end, XGBoost, Logistic Regression and Random Forest Classficator where chosen for further hyper-tuning, as they performed best as is among 5 initial models. In case of Random Forest Classificator it increased the accuracy the most, from 7 initial misclassifications to 4, effectively placing it better than both Logistic Regression and XGBoost with 6 and 5 misclassifications respectively"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d1ee881",
   "metadata": {
    "papermill": {
     "duration": 0.009423,
     "end_time": "2023-07-11T07:55:07.825930",
     "exception": false,
     "start_time": "2023-07-11T07:55:07.816507",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 2.1 Training models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "772ca885",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-07-11T07:55:07.848222Z",
     "iopub.status.busy": "2023-07-11T07:55:07.847374Z",
     "iopub.status.idle": "2023-07-11T07:55:08.531629Z",
     "shell.execute_reply": "2023-07-11T07:55:08.529578Z"
    },
    "papermill": {
     "duration": 0.699434,
     "end_time": "2023-07-11T07:55:08.535021",
     "exception": false,
     "start_time": "2023-07-11T07:55:07.835587",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of Logistic Regression: 0.9512195121951219\n",
      "Confusion Matrix:\n",
      "[[103   2]\n",
      " [  4  14]]\n",
      "Accuracy of Random Forest Clasifier: 0.943089430894309\n",
      "Confusion Matrix:\n",
      "[[104   1]\n",
      " [  6  12]]\n",
      "Accuracy of Support Vector Machine: 0.926829268292683\n",
      "Confusion Matrix:\n",
      "[[105   0]\n",
      " [  9   9]]\n",
      "Accuracy of Naive Bayes: 0.8861788617886179\n",
      "Confusion Matrix:\n",
      "[[102   3]\n",
      " [ 11   7]]\n",
      "Accuracy of XGBoost: 0.959349593495935\n",
      "Confusion Matrix:\n",
      "[[102   3]\n",
      " [  2  16]]\n"
     ]
    }
   ],
   "source": [
    "logreg = LogisticRegression()\n",
    "\n",
    "# Fit the model to the training data\n",
    "logreg.fit(X_train, y_train)\n",
    "\n",
    "#logistic_regression.fit(X_train, y_train)\n",
    "\n",
    "# Predict the target variable for the test set\n",
    "y_pred = logreg.predict(X_test)\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy_lr = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy of Logistic Regression:\", accuracy_lr)\n",
    "\n",
    "# Print confusion matrix\n",
    "confusion_matrix_lr = confusion_matrix(y_test, y_pred)\n",
    "print(\"Confusion Matrix:\")\n",
    "print(confusion_matrix_lr)\n",
    "\n",
    "\n",
    "random_forest_model = RandomForestClassifier()\n",
    "\n",
    "# Train the model on the training data\n",
    "random_forest_model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test data\n",
    "y_pred_rf = random_forest_model.predict(X_test)\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy_rf = accuracy_score(y_test, y_pred_rf)\n",
    "print(\"Accuracy of Random Forest Clasifier:\", accuracy_rf)\n",
    "\n",
    "# Print confusion matrix\n",
    "confusion_matrix_rf = confusion_matrix(y_test, y_pred_rf)\n",
    "print(\"Confusion Matrix:\")\n",
    "print(confusion_matrix_rf)\n",
    "\n",
    "# Initialize the SVC classifier\n",
    "svc_model = SVC(kernel='rbf')\n",
    "# Train the SVC model on the training data\n",
    "svc_model.fit(X_train, y_train)\n",
    "\n",
    "# Initialize the GaussianNB classifier\n",
    "gnb_model = GaussianNB()\n",
    "# Train the GaussianNB model on the training data\n",
    "gnb_model.fit(X_train, y_train)\n",
    "\n",
    "# Initialize the XGBClassifier\n",
    "xgb_model = XGBClassifier()\n",
    "# Train the XGBClassifier on the training data\n",
    "xgb_model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test data\n",
    "y_pred_svc = svc_model.predict(X_test)\n",
    "y_pred_gnb = gnb_model.predict(X_test)\n",
    "y_pred_xgb = xgb_model.predict(X_test)\n",
    "\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy_svc = accuracy_score(y_test, y_pred_svc)\n",
    "print(\"Accuracy of Support Vector Machine:\", accuracy_svc)\n",
    "\n",
    "\n",
    "confusion_matrix_svc = confusion_matrix(y_test, y_pred_svc)\n",
    "print(\"Confusion Matrix:\")\n",
    "print(confusion_matrix_svc)\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy_gnb = accuracy_score(y_test, y_pred_gnb)\n",
    "print(\"Accuracy of Naive Bayes:\", accuracy_gnb)\n",
    "\n",
    "\n",
    "confusion_matrix_gnb = confusion_matrix(y_test, y_pred_gnb)\n",
    "print(\"Confusion Matrix:\")\n",
    "print(confusion_matrix_gnb)\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy_xgb = accuracy_score(y_test, y_pred_xgb)\n",
    "print(\"Accuracy of XGBoost:\", accuracy_xgb)\n",
    "\n",
    "\n",
    "confusion_matrix_xgb = confusion_matrix(y_test, y_pred_xgb)\n",
    "print(\"Confusion Matrix:\")\n",
    "print(confusion_matrix_xgb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61602816",
   "metadata": {
    "papermill": {
     "duration": 0.010703,
     "end_time": "2023-07-11T07:55:08.557236",
     "exception": false,
     "start_time": "2023-07-11T07:55:08.546533",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 2.2 Fine-tuning top 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7bb82d2b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-07-11T07:55:08.579787Z",
     "iopub.status.busy": "2023-07-11T07:55:08.579372Z",
     "iopub.status.idle": "2023-07-11T07:56:38.493723Z",
     "shell.execute_reply": "2023-07-11T07:56:38.492179Z"
    },
    "papermill": {
     "duration": 89.938626,
     "end_time": "2023-07-11T07:56:38.506303",
     "exception": false,
     "start_time": "2023-07-11T07:55:08.567677",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of best Random Forest: 0.967479674796748\n",
      "Confusion Matrix:\n",
      "[[104   1]\n",
      " [  3  15]]\n"
     ]
    }
   ],
   "source": [
    "# Initialize the Random Forest classifier\n",
    "random_forest_best = RandomForestClassifier()\n",
    "\n",
    "# Define the hyperparameters and their possible values\n",
    "param_grid_rf_best = {\n",
    "    'n_estimators': [100, 200, 300],  # Number of trees in the forest\n",
    "    'max_depth': [None, 5, 10],       # Maximum depth of the trees\n",
    "    'min_samples_split': [2, 5, 10],  # Minimum number of samples required to split an internal node\n",
    "}\n",
    "\n",
    "# Perform grid search using 5-fold cross-validation\n",
    "grid_search_rf_best = GridSearchCV(random_forest_best, param_grid_rf_best, cv=5)\n",
    "\n",
    "# Fit the grid search to the training data\n",
    "grid_search_rf_best.fit(X_train, y_train)\n",
    "\n",
    "# Get the best parameters and the best model\n",
    "best_params_rf_best = grid_search_rf_best.best_params_\n",
    "best_model_rf_best = grid_search_rf_best.best_estimator_\n",
    "\n",
    "# Make predictions on the test data using the best model\n",
    "y_pred_rf_best = best_model_rf_best.predict(X_test)\n",
    "\n",
    "accuracy_rf_best = accuracy_score(y_test, y_pred_rf_best)\n",
    "print(\"Accuracy of best Random Forest:\", accuracy_rf_best)\n",
    "\n",
    "confusion_matrix_rf_best = confusion_matrix(y_test, y_pred_rf_best)\n",
    "print(\"Confusion Matrix:\")\n",
    "print(confusion_matrix_rf_best)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "43edbd27",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-07-11T07:56:38.529649Z",
     "iopub.status.busy": "2023-07-11T07:56:38.529194Z",
     "iopub.status.idle": "2023-07-11T07:57:20.934001Z",
     "shell.execute_reply": "2023-07-11T07:57:20.933094Z"
    },
    "papermill": {
     "duration": 42.431576,
     "end_time": "2023-07-11T07:57:20.948302",
     "exception": false,
     "start_time": "2023-07-11T07:56:38.516726",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of best XGBoost: 0.959349593495935\n",
      "Confusion Matrix:\n",
      "[[102   3]\n",
      " [  2  16]]\n"
     ]
    }
   ],
   "source": [
    "# Initialize the XGBoost classifier\n",
    "xgb_model_best = XGBClassifier()\n",
    "\n",
    "# Define the hyperparameters and their possible values\n",
    "param_grid_xgb_best = {\n",
    "    'n_estimators': [100, 200, 300],  # Number of trees in the ensemble\n",
    "    'max_depth': [3, 5, 7],           # Maximum depth of each tree\n",
    "    'learning_rate': [0.1, 0.01, 0.001],  # Learning rate\n",
    "}\n",
    "\n",
    "# Perform grid search using 5-fold cross-validation\n",
    "grid_search_xgb_best = GridSearchCV(xgb_model_best, param_grid_xgb_best, cv=5)\n",
    "\n",
    "# Fit the grid search to the training data\n",
    "grid_search_xgb_best.fit(X_train, y_train)\n",
    "\n",
    "# Get the best parameters and the best model\n",
    "best_params_xgb = grid_search_xgb_best.best_params_\n",
    "best_model_xgb = grid_search_xgb_best.best_estimator_\n",
    "\n",
    "# Make predictions on the test data using the best model\n",
    "y_pred_xgb_best = best_model_xgb.predict(X_test)\n",
    "\n",
    "accuracy_xgb_best = accuracy_score(y_test, y_pred_xgb_best)\n",
    "print(\"Accuracy of best XGBoost:\", accuracy_xgb_best)\n",
    "\n",
    "confusion_matrix_xgb_best = confusion_matrix(y_test, y_pred_xgb_best)\n",
    "print(\"Confusion Matrix:\")\n",
    "print(confusion_matrix_xgb_best)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7616fa1a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-07-11T07:57:20.972742Z",
     "iopub.status.busy": "2023-07-11T07:57:20.972297Z",
     "iopub.status.idle": "2023-07-11T07:57:21.347711Z",
     "shell.execute_reply": "2023-07-11T07:57:21.346197Z"
    },
    "papermill": {
     "duration": 0.391198,
     "end_time": "2023-07-11T07:57:21.350417",
     "exception": false,
     "start_time": "2023-07-11T07:57:20.959219",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of best Logistic Regression: 0.9512195121951219\n",
      "Confusion Matrix:\n",
      "[[103   2]\n",
      " [  4  14]]\n"
     ]
    }
   ],
   "source": [
    "# Initialize the Logistic Regression classifier\n",
    "logreg_model_best = LogisticRegression()\n",
    "\n",
    "# Define the hyperparameters and their possible values\n",
    "param_grid_lr_best = {\n",
    "    'C': [0.1, 1.0, 10.0],      # Inverse of regularization strength\n",
    "    'penalty': ['l1', 'l2'],    # Regularization penalty type\n",
    "    'solver': ['liblinear'],    # Algorithm to use in the optimization problem\n",
    "}\n",
    "\n",
    "# Perform grid search using 5-fold cross-validation\n",
    "grid_search_lr_best = GridSearchCV(logreg_model_best, param_grid_lr_best, cv=5)\n",
    "\n",
    "# Fit the grid search to the training data\n",
    "grid_search_lr_best.fit(X_train, y_train)\n",
    "\n",
    "# Get the best parameters and the best model\n",
    "best_params_lr = grid_search_lr_best.best_params_\n",
    "best_model_lr = grid_search_lr_best.best_estimator_\n",
    "\n",
    "# Make predictions on the test data using the best model\n",
    "y_pred_lr_best = best_model_lr.predict(X_test)\n",
    "\n",
    "accuracy_lr_best = accuracy_score(y_test, y_pred_lr_best)\n",
    "print(\"Accuracy of best Logistic Regression:\", accuracy_lr_best)\n",
    "\n",
    "confusion_matrix_lr_best = confusion_matrix(y_test, y_pred_lr_best)\n",
    "print(\"Confusion Matrix:\")\n",
    "print(confusion_matrix_lr_best)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f0d6a51",
   "metadata": {},
   "source": [
    "# 3 Conclusion\n",
    "In conclusion, Random Forest Classificator has been established as the most effective in this set, however, further testing must be done. Additionally, different penalty strategies for the model should be discussed with the healthcare providers, as it might be the case that it is much more vital to catch people with the disease even if it means going though more healthy people as well. As for other models, it might deem useful to test Deep Learning architectures that can spot more synergistic between features, rather than their effect on the result alone."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c27295cb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 152.214428,
   "end_time": "2023-07-11T07:57:22.874503",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2023-07-11T07:54:50.660075",
   "version": "2.4.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
